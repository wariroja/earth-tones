{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('cleaned_data.csv')\n",
    "data.rename(columns={' amazement':'amazement', \n",
    "                     ' solemnity':'solemnity', \n",
    "                     ' tenderness':'tenderness', \n",
    "                     ' nostalgia':'nostalgia',\n",
    "                     ' calmness':'calmness', \n",
    "                     ' power': 'power', \n",
    "                     ' joyful_activation':'joyful_activation', \n",
    "                     ' tension':'tension', \n",
    "                     ' sadness':'sadness',\n",
    "                     ' mood':'mood', \n",
    "                     ' age':'age'}, inplace=True)\n",
    "data = data.drop(['Unnamed: 0', 'track id', ' liked', ' disliked', ' gender', ' mother tongue', 'merge_key', 'Title',\n",
    "       'Artist','file_id'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "genres = ['Pop', 'Rock', 'Jazz', 'Classical', 'Electronic', 'Hip-hop'] \n",
    "bpm_range = (60, 200)\n",
    "listening_frequencies = ['hourly', 'daily', 'weekly', 'monthly']\n",
    "\n",
    "data['avg_bpm_listened'] = [random.randint(bpm_range[0], bpm_range[1]) for _ in range(len(data))]\n",
    "data['most_listened_genre'] = [random.choice(genres) for _ in range(len(data))]\n",
    "data['listening_frequency'] = [random.choice(listening_frequencies) for _ in range(len(data))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_vector_columns = [\n",
    "    'amazement', 'solemnity', 'tenderness', 'nostalgia', 'calmness', 'power',\n",
    "    'joyful_activation', 'tension', 'sadness', 'mood', 'age', \n",
    "    'avg_bpm_listened', 'most_listened_genre', 'listening_frequency'\n",
    "]\n",
    "input_vector = data[input_vector_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For 'genre' and 'key', we convert to a long tensor of class indices, not one-hot.\n",
    "# For 'bpm', we keep the normalized values as they are used for regression.\n",
    "\n",
    "genre = data['Genre']\n",
    "key = data['Key']\n",
    "bpm = data['BPM']\n",
    "\n",
    "# Assuming 'genre' and 'key' are categorical data, you convert them to corresponding indices.\n",
    "# This process is known as label encoding. The LabelEncoder from sklearn can help with this.\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Label encode genre and key\n",
    "genre_encoder = LabelEncoder()\n",
    "genre_labels = genre_encoder.fit_transform(genre)\n",
    "\n",
    "key_encoder = LabelEncoder()\n",
    "key_labels = key_encoder.fit_transform(key)\n",
    "\n",
    "# Normalize bpm as it's a continuous value\n",
    "bpm_values = bpm.values.reshape(-1, 1)  # Reshaping to conform to the scaler's requirements\n",
    "scaler = MinMaxScaler()\n",
    "bpm_normalized = scaler.fit_transform(bpm_values)\n",
    "\n",
    "# Convert all to PyTorch tensors\n",
    "genre_tensor = torch.tensor(genre_labels, dtype=torch.long)  # Labels are indices, so they should be long tensors\n",
    "key_tensor = torch.tensor(key_labels, dtype=torch.long)  # Same for these labels\n",
    "bpm_tensor = torch.tensor(bpm_normalized, dtype=torch.float32)  # This remains a float tensor\n",
    "\n",
    "targets = {\n",
    "    \"genre\": genre_tensor,\n",
    "    \"key\": key_tensor,\n",
    "    \"bpm\": bpm_tensor\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_features = input_vector[['amazement', 'solemnity', 'tenderness', 'nostalgia', 'calmness', 'power', 'joyful_activation', 'tension', 'sadness']]\n",
    "categorical_features = input_vector[['most_listened_genre', 'listening_frequency']]\n",
    "continuous_features = input_vector[['mood', 'age', 'avg_bpm_listened']]\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "continuous_normalized = scaler.fit_transform(continuous_features)\n",
    "\n",
    "binary_df = pd.DataFrame(binary_features, dtype=np.float32)\n",
    "continuous_normalized_df = pd.DataFrame(continuous_normalized, columns=continuous_features.columns)\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoders = {} \n",
    "categorical_features_encoded = categorical_features.copy() \n",
    "\n",
    "for column in categorical_features.columns:\n",
    "    le = LabelEncoder()\n",
    "    categorical_features_encoded[column] = le.fit_transform(categorical_features[column])\n",
    "    label_encoders[column] = le  \n",
    "\n",
    "# Combine all the features into a single DataFrame\n",
    "input_vector_processed = pd.concat([binary_df, continuous_normalized_df, categorical_features_encoded], axis=1)\n",
    "\n",
    "# Convert the combined DataFrame into a tensor\n",
    "input_tensor = torch.tensor(input_vector_processed.values, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MultiOutputModel(nn.Module):\n",
    "    def __init__(self, num_binary, num_continuous, num_genre_labels, num_key_labels):\n",
    "        super(MultiOutputModel, self).__init__()\n",
    "\n",
    "        # Total number of features after concatenation\n",
    "        total_num_features = num_binary + num_continuous + 2\n",
    "\n",
    "        # Common layers\n",
    "        self.fc1 = nn.Linear(total_num_features, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "\n",
    "        # Task-specific layers\n",
    "        self.fc_genre = nn.Linear(32, num_genre_labels)\n",
    "        self.fc_key = nn.Linear(32, num_key_labels)\n",
    "        self.fc_bpm = nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, binary_data, continuous_data, categorical_data):\n",
    "        # Concatenate all features into a single large input vector\n",
    "        x = torch.cat([binary_data, continuous_data, categorical_data], dim=1)  # Concatenate along feature dimension\n",
    "\n",
    "        # Common layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "\n",
    "        # Task-specific layers\n",
    "        out_genre = self.fc_genre(x)\n",
    "        out_key = self.fc_key(x)\n",
    "        out_bpm = self.fc_bpm(x)\n",
    "\n",
    "        # Dictionary of outputs\n",
    "        outputs = {\n",
    "            \"genre\": out_genre,\n",
    "            \"key\": out_key,\n",
    "            \"bpm\": out_bpm\n",
    "        }\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torch\n",
    "\n",
    "binary_tensor = torch.tensor(binary_df.values, dtype=torch.float32)\n",
    "non_categorical_tensor = torch.tensor(continuous_normalized_df.values, dtype=torch.float32)\n",
    "categorical_tensor = torch.tensor(categorical_features_encoded.values, dtype=torch.long)\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, inputs_binary, inputs_non_cat, inputs_cat, targets):\n",
    "        self.inputs_binary = inputs_binary\n",
    "        self.inputs_non_cat = inputs_non_cat\n",
    "        self.inputs_cat = inputs_cat\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.targets['genre'])  # assuming length is same for all targets\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        binary = self.inputs_binary[idx]\n",
    "        non_cat = self.inputs_non_cat[idx]\n",
    "        cat = self.inputs_cat[idx]\n",
    "        target = {key: val[idx] for key, val in self.targets.items()}\n",
    "        return binary, non_cat, cat, target\n",
    "\n",
    "# Combine all inputs and targets into a single dataset\n",
    "full_dataset = CustomDataset(\n",
    "    inputs_binary=binary_tensor,\n",
    "    inputs_non_cat=non_categorical_tensor,\n",
    "    inputs_cat=categorical_tensor,\n",
    "    targets=targets\n",
    ")\n",
    "\n",
    "# Define a size for your validation set\n",
    "val_size = int(len(full_dataset) * 0.1)  # 10% for validation\n",
    "train_size = len(full_dataset) - val_size\n",
    "\n",
    "# Randomly split the dataset into training and validation sets\n",
    "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "# Create DataLoader instances\n",
    "batch_size = 32  # You can adjust this number as needed\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_binary = len(binary_features.columns)  # This is based on your binary_features DataFrame\n",
    "num_continuous = len(continuous_features.columns)  # This is based on your continuous_features DataFrame\n",
    "\n",
    "# Step 3: Define the number of unique labels for your target variables.\n",
    "num_genre_labels = 4  \n",
    "num_key_labels = 24\n",
    "\n",
    "# Step 4: Instantiate your model with all this information\n",
    "model = MultiOutputModel(\n",
    "    num_binary=num_binary, \n",
    "    num_continuous=num_continuous, \n",
    "    num_genre_labels=num_genre_labels, \n",
    "    num_key_labels=num_key_labels\n",
    ")\n",
    "\n",
    "def train_model(model, criteria, optimizer, train_loader, epochs=10):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "\n",
    "        for binary_data, categorical_data, non_categorical_data, batch_targets in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Unpack targets\n",
    "            targets_genre = batch_targets[\"genre\"]\n",
    "            targets_key = batch_targets[\"key\"]\n",
    "            targets_bpm = batch_targets[\"bpm\"]\n",
    "\n",
    "            # Forward pass through the model\n",
    "            predictions = model(binary_data, non_categorical_data, categorical_data)\n",
    "\n",
    "            # Calculate losses for each task\n",
    "            loss_genre = criteria[\"genre\"](predictions[\"genre\"], targets_genre)  \n",
    "            loss_key = criteria[\"key\"](predictions[\"key\"], targets_key)  \n",
    "            loss_bpm = criteria[\"bpm\"](predictions[\"bpm\"], targets_bpm)\n",
    "\n",
    "            # Combine the losses as needed, usually by summing them\n",
    "            loss = loss_genre + loss_key + loss_bpm\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        # Print loss statistics\n",
    "        print(f\"Epoch {epoch+1}, Loss: {total_loss / len(train_loader)}\")\n",
    "\n",
    "# Assuming you already have the model and optimizer defined, as well as the train_loader prepared\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Define separate criteria for each task (assuming genre and key are classification, bpm is regression)\n",
    "criteria = {\n",
    "    \"genre\": nn.CrossEntropyLoss(),  # for classification tasks\n",
    "    \"key\": nn.CrossEntropyLoss(),    # for classification tasks\n",
    "    \"bpm\": nn.MSELoss()  # since bpm is a continuous value, we use Mean Squared Error Loss\n",
    "}\n",
    "\n",
    "# Now, you can call your training function with all necessary parameters\n",
    "train_model(model, criteria, optimizer, train_loader, epochs=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
